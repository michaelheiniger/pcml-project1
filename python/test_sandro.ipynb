{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from proj1_helpers import *\n",
    "from helpers import batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'D:/dev/EPFL/Machine Learning/Project 1/Repo/data/train.csv'\n",
    "\n",
    "Y, X, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "df = pd.read_csv(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.22474487, -1.22474487],\n",
       "       [ 1.22474487,  1.22474487],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MISSING_VAL = -999\n",
    "\n",
    "def clean_data(x):\n",
    "    \"\"\" \n",
    "    This function performs cleaning of the data.\n",
    "    It replaces missing values in a column with the mean of non missing values in the same column\n",
    "    \"\"\"\n",
    "    # This function finds all the missing values in the given vector\n",
    "    missing = lambda x: np.abs(np.subtract(x, MISSING_VAL)) < 1e-8\n",
    "        \n",
    "    # for each column of x, we calculate the mean of non missing values (value that is not equal to MISSING_VAL).    \n",
    "    x_mean = [np.mean(x[~missing(x[:, i]), i]) for i in range(x.shape[1])]\n",
    "    \n",
    "    # for each column of x, we replace missing values with the corresponding mean of that column calculated above.\n",
    "    for i in range(x.shape[1]):\n",
    "        x[missing(x[:, i]), i] = x_mean[i]\n",
    "    \n",
    "    return np.array(x)\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"This function normalizes the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def clean_and_normalize(x):\n",
    "    return normalize(clean_data(x))\n",
    "    \n",
    "x, mean_x, std_x = clean_and_normalize(np.array([[1, 1],[2, 2], [1.5, 1.5]], dtype=np.float64))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9292726 ,  0.04457539,  0.51509592, ..., -0.46918386,\n",
       "        -0.50704195,  0.67478267],\n",
       "       [ 1.15822487,  0.21896718,  0.5702067 , ..., -0.48194091,\n",
       "        -0.48183633, -0.01074939],\n",
       "       [ 0.75999168,  1.17081027,  0.80171682, ..., -0.48194091,\n",
       "        -0.48183633, -0.03087583],\n",
       "       ..., \n",
       "       [ 0.59285022,  0.13497623,  0.29102494, ..., -0.48194091,\n",
       "        -0.48183633, -0.0538964 ],\n",
       "       [ 0.48578774, -0.28450975,  0.21941557, ..., -0.48194091,\n",
       "        -0.48183633, -0.4818202 ],\n",
       "       [ 0.75999168,  0.25960731,  0.2399904 , ..., -0.48194091,\n",
       "        -0.48183633, -0.4818202 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = clean_and_normalize(X)[0]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X[0:1000,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(X, columns=df.columns.drop([\"Id\", \"Prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outputs persentage of missing values (-999) per column\n",
    "(df[df.columns.drop([\"Id\", \"Prediction\"])].where(df == -999).count() / df.shape[0] * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(1000).where(df > -999).plot(subplots=True, kind='line', figsize=(12, 200))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.head(1000).plot(subplots=True, kind='line', figsize=(12, 200))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df.columns.drop([\"Id\"])].head(1000).where(df > -999).plot(kind='box', figsize=(14, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return np.divide(1, np.add(1, np.exp(-t)))\n",
    "\n",
    "\n",
    "def compute_log_likelihood(y, tx, w):\n",
    "    loss = 0\n",
    "    for i in range(tx.shape[0]):\n",
    "        #x_t = tx[i].transpose()\n",
    "        y_est = tx[i].dot(w)\n",
    "        loss = loss + np.log(1 + np.exp(y_est)) - y[i] * y_est\n",
    "    return loss\n",
    "\n",
    "def compute_log_likelihood(y, tx, w):\n",
    "    loss = 0\n",
    "    for i in range(tx.shape[0]):\n",
    "        #x_t = tx[i].transpose()\n",
    "        y_est = tx[i].dot(w)\n",
    "        loss = loss + np.log(1 + np.exp(y_est)) - y[i] * y_est\n",
    "    return loss\n",
    "\n",
    "def compute_log_likelihood_penalized(y, tx, w, lambda_):\n",
    "    loss = 0\n",
    "    for i in range(tx.shape[0]):\n",
    "        #x_t = tx[i].transpose()\n",
    "        y_est = tx[i].dot(w)\n",
    "        loss = loss + np.log(1 + np.exp(y_est)) - y[i] * y_est\n",
    "    return np.add(loss, lambda_ * w.transpose().dot(w))\n",
    "\n",
    "def compute_gradient_log_likelihood(y, tx, w):\n",
    "    \"\"\"Computes gradient of the max likelihood estimator for logistic regression\"\"\"\n",
    "    xt_t = tx.transpose()\n",
    "    return xt_t.dot((sigmoid(tx.dot(w)) - y))\n",
    "\n",
    "def compute_gradient_log_likelihood_penalized(y, tx, w, lambda_):\n",
    "    \"\"\"Computes gradient of the max likelihood estimator for logistic regression\"\"\"\n",
    "    xt_t = tx.transpose()\n",
    "    gradient = xt_t.dot((sigmoid(tx.dot(w)) - y))\n",
    "    return np.add(gradient, np.multiply(2 * lambda_, w))\n",
    "\n",
    "def logistic_regression_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\" Logistic regression using Schocastic gradient descent algorithm \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    losses = []\n",
    "\n",
    "    np.seterr(all='print')\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "        y_batch, tx_batch = next(batch_iter(y, tx, batch_size, num_batches=1, shuffle=True))\n",
    "        \n",
    "        grad, loss = compute_gradient_log_likelihood_penalized(y_batch, tx_batch, w, 0.1), compute_log_likelihood_penalized(y_batch, tx_batch, w, 0.1)\n",
    "        \n",
    "        w = np.subtract(w, np.multiply(gamma, grad))\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        \n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < 1e-8:\n",
    "            break  \n",
    "        \n",
    "    return w\n",
    "\n",
    "#X = X[0]\n",
    "tx = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "Y = Y.reshape(len(Y), 1)\n",
    "ww = logistic_regression_SGD(Y, tx, initial_w, 250, 100, 1e-8)\n",
    "\n",
    "ww\n",
    "#compute_gradient_log_likelihood(Y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen(n):\n",
    "    for i in range(n):\n",
    "        yield i + 1\n",
    "        \n",
    "for i in gen(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[[ 173.28679514]]\n",
      "Current iteration=10, the loss=[[ 173.21021542]]\n",
      "Current iteration=20, the loss=[[ 173.16435761]]\n",
      "Current iteration=30, the loss=[[ 173.10587582]]\n",
      "Current iteration=40, the loss=[[ 172.97204096]]\n",
      "Current iteration=50, the loss=[[ 172.95048784]]\n",
      "Current iteration=60, the loss=[[ 172.87980481]]\n",
      "Current iteration=70, the loss=[[ 172.80809548]]\n",
      "Current iteration=80, the loss=[[ 172.803401]]\n",
      "Current iteration=90, the loss=[[ 172.53083483]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -2.05302262e-04],\n",
       "       [ -1.54855014e-04],\n",
       "       [ -3.39342506e-05],\n",
       "       [ -7.20150841e-05],\n",
       "       [  7.32317275e-06],\n",
       "       [  9.43150388e-05],\n",
       "       [ -5.85027547e-04],\n",
       "       [  9.98724575e-05],\n",
       "       [  9.39671750e-05],\n",
       "       [  5.83058739e-05],\n",
       "       [ -1.89884806e-04],\n",
       "       [  9.55094679e-05],\n",
       "       [  9.99402458e-05],\n",
       "       [  9.80513185e-05],\n",
       "       [  3.10243819e-05],\n",
       "       [  9.89181725e-05],\n",
       "       [  9.89187151e-05],\n",
       "       [ -4.64893772e-07],\n",
       "       [  9.89843176e-05],\n",
       "       [  9.88625697e-05],\n",
       "       [  1.35959786e-05],\n",
       "       [  9.90343176e-05],\n",
       "       [ -2.98437868e-04],\n",
       "       [  9.71860424e-05],\n",
       "       [ -6.77071484e-05],\n",
       "       [  9.89160245e-05],\n",
       "       [  9.89433543e-05],\n",
       "       [ -2.27648851e-05],\n",
       "       [  9.89569763e-05],\n",
       "       [  9.89281484e-05],\n",
       "       [ -2.26067453e-05]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logistic_regression as lr\n",
    "\n",
    "\n",
    "tx = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "Y = Y.reshape(len(Y), 1)\n",
    "ww = lr.logistic_regression_SGD(Y, tx, initial_w, 250, 100, 1e-8, 0.1)\n",
    "\n",
    "ww"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
