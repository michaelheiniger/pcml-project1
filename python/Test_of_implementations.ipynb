{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from data_modification import replace_by_mean\n",
    "\n",
    "\"\"\" Load TRAINING data \"\"\"\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, raw_tx, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "# Replace -999 by the mean of its respective column\n",
    "processed_tx = replace_by_mean(raw_tx)\n",
    "\n",
    "# Standardize (subtract mean and divive by standard deviation)\n",
    "processed_tx,mean_pr_tx,std_pr_tx = standardize(processed_tx, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Least squares with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [ -2.10143501e-01   2.51724232e-03  -2.60505491e-01  -2.46137310e-01\n",
      "  -1.82979695e-02  -8.23612150e-03   1.00697248e-01   1.85207973e-03\n",
      "   2.64019458e-01  -2.80149795e-02   1.00018767e-01  -1.79183852e-01\n",
      "   1.22953669e-01   9.61122113e-02   1.64255318e-01   4.57586102e-04\n",
      "   2.00926577e-03   2.57819491e-01  -4.32891513e-05   5.72497372e-03\n",
      "   1.05679970e-01   3.47645245e-03  -5.54607535e-02  -2.12954943e-02\n",
      "  -8.70220705e-02   2.79081053e-03   4.29395177e-03  -7.11935655e-02\n",
      "   4.30635863e-03   1.84018336e-03   2.67614434e-02]\n",
      "\n",
      "\n",
      "Loss:\n",
      " 85200.9610964\n"
     ]
    }
   ],
   "source": [
    "from implementations import least_squares_GD\n",
    "\n",
    "initial_w = np.zeros((processed_tx.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 0.01\n",
    "\n",
    "w, loss = least_squares_GD(y, processed_tx, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"\\n\")\n",
    "print(\"Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [ 0.01316851 -0.0328513  -0.1032939  -0.0039369  -0.02081322  0.00811093\n",
      " -0.04822099 -0.01961342  0.0109631  -0.05058128  0.00184863 -0.10209009\n",
      " -0.06204941  0.05566102  0.11644048 -0.02014612 -0.0894478  -0.03129034\n",
      " -0.0367632   0.11528398 -0.04197272  0.05580443  0.04639581  0.00277032\n",
      " -0.01292013 -0.1077839  -0.0926331   0.01832733  0.05634268 -0.07242884\n",
      " -0.01766777]\n",
      "\n",
      "\n",
      "Loss:\n",
      " 0.129235233693\n"
     ]
    }
   ],
   "source": [
    "from implementations import least_squares_SGD\n",
    "\n",
    "initial_w = np.zeros((processed_tx.shape[1],1))\n",
    "max_iters = 50\n",
    "gamma = 0.01\n",
    "\n",
    "w, loss = least_squares_SGD(y, processed_tx, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"\\n\")\n",
    "print(\"Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [  5.20696559e+00   9.63458006e-03  -2.54719226e-01  -2.63502969e-01\n",
      "  -1.10181076e-03   2.18423835e-02   9.00537809e-02   4.83490442e-03\n",
      "   2.82008766e-01  -2.81502573e-02  -3.29326782e+02  -1.88141152e-01\n",
      "   1.18065031e-01   7.66172583e-02   6.39754849e+01  -7.79460340e-04\n",
      "  -8.30656871e-04   6.30911711e+01  -8.61169078e-04   2.51791383e-03\n",
      "   1.03659310e-01   9.33785709e-04  -4.70019042e-02   4.17575954e-02\n",
      "  -4.75783451e-02   6.50726186e-04   1.88755821e-04  -3.66001839e-02\n",
      "   1.55837347e-03  -1.74318742e-03   2.78984473e+02]\n",
      "\n",
      "\n",
      "Loss:\n",
      " 85102.3630404\n"
     ]
    }
   ],
   "source": [
    "from implementations import least_squares\n",
    "\n",
    "w, loss = least_squares(y, processed_tx)\n",
    "\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"\\n\")\n",
    "print(\"Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [-0.0280887  -0.01393268 -0.20013708 -0.07221326  0.03795443  0.01984449\n",
      "  0.05542273 -0.01392809  0.12000181 -0.02879156  0.03820549 -0.08841178\n",
      "  0.11451565  0.08757881  0.11868554  0.00115836  0.00229579  0.07669044\n",
      "  0.00128682  0.00685682  0.02623058  0.00548248 -0.01622494  0.00603481\n",
      " -0.02317321  0.00365795  0.0062076  -0.05102699  0.00572182  0.00429357\n",
      "  0.00127156]\n",
      "\n",
      "\n",
      "Loss:\n",
      " 0.349444846357\n"
     ]
    }
   ],
   "source": [
    "from implementations import ridge_regression\n",
    "\n",
    "lambda_ = 0.1\n",
    "\n",
    "w, loss = ridge_regression(y, processed_tx, lambda_)\n",
    "\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"\\n\")\n",
    "print(\"Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 1)\n"
     ]
    }
   ],
   "source": [
    "def convert_minus_one_to_zero(x):\n",
    "    if x == -1:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "# Convert the values -1 to 0 in the vector y: needed for logistic regression\n",
    "y_for_log = list(map(convert_minus_one_to_zero, y))\n",
    "y_for_log = np.asarray(y_for_log)\n",
    "y_for_log = y_for_log[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=69.31471805599453\n",
      "Current iteration=10, the loss=66.42680182433267\n",
      "Current iteration=20, the loss=65.70465981903608\n",
      "Current iteration=30, the loss=57.144467051331944\n",
      "Current iteration=40, the loss=64.97350005596627\n",
      "Weights:\n",
      " [[-0.01228004]\n",
      " [-0.02064572]\n",
      " [-0.08523098]\n",
      " [-0.02281858]\n",
      " [ 0.03221598]\n",
      " [ 0.02011347]\n",
      " [ 0.01918041]\n",
      " [-0.02182487]\n",
      " [-0.01673321]\n",
      " [-0.01188065]\n",
      " [ 0.0173557 ]\n",
      " [-0.0493835 ]\n",
      " [ 0.0675026 ]\n",
      " [ 0.0680756 ]\n",
      " [ 0.03684481]\n",
      " [ 0.00338647]\n",
      " [ 0.00691451]\n",
      " [-0.02923728]\n",
      " [ 0.00594401]\n",
      " [ 0.00540324]\n",
      " [-0.00612073]\n",
      " [ 0.00827062]\n",
      " [ 0.0081356 ]\n",
      " [ 0.02767576]\n",
      " [-0.0028494 ]\n",
      " [ 0.00521155]\n",
      " [ 0.01035072]\n",
      " [-0.04254888]\n",
      " [ 0.01026975]\n",
      " [ 0.0126743 ]\n",
      " [ 0.01889566]]\n",
      "\n",
      "\n",
      "Loss:\n",
      " 62.0319607614\n"
     ]
    }
   ],
   "source": [
    "from implementations import logistic_regression\n",
    "\n",
    "initial_w = np.zeros((processed_tx.shape[1],1))\n",
    "max_iters = 50\n",
    "gamma = 0.0001\n",
    "\n",
    "w, loss = logistic_regression(y_for_log, processed_tx, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"\\n\")\n",
    "print(\"Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[[ 69.31471806]]\n",
      "Current iteration=10, the loss=[[ 60.92563724]]\n",
      "Current iteration=20, the loss=[[ 52.48830775]]\n",
      "Current iteration=30, the loss=[[ 52.20305997]]\n",
      "Current iteration=40, the loss=[[ 60.56446454]]\n",
      "Current iteration=50, the loss=[[ 55.85920878]]\n",
      "Current iteration=60, the loss=[[ 54.16333147]]\n",
      "Current iteration=70, the loss=[[ 57.33694975]]\n",
      "Current iteration=80, the loss=[[ 43.05055396]]\n",
      "Current iteration=90, the loss=[[ 50.81126253]]\n",
      "Current iteration=100, the loss=[[ 59.10704957]]\n",
      "Current iteration=110, the loss=[[ 58.91900941]]\n",
      "Current iteration=120, the loss=[[ 46.8373547]]\n",
      "Current iteration=130, the loss=[[ 50.26477548]]\n",
      "Current iteration=140, the loss=[[ 53.20376672]]\n",
      "Current iteration=150, the loss=[[ 44.41899943]]\n",
      "Current iteration=160, the loss=[[ 38.44757634]]\n",
      "Current iteration=170, the loss=[[ 52.01632966]]\n",
      "Current iteration=180, the loss=[[ 63.13479765]]\n",
      "Current iteration=190, the loss=[[ 52.12464134]]\n",
      "Current iteration=200, the loss=[[ 60.32766479]]\n",
      "Current iteration=210, the loss=[[ 53.06511565]]\n",
      "Current iteration=220, the loss=[[ 56.03762821]]\n",
      "Current iteration=230, the loss=[[ 47.77499489]]\n",
      "Current iteration=240, the loss=[[ 52.17364734]]\n",
      "Current iteration=250, the loss=[[ 51.4428724]]\n",
      "Current iteration=260, the loss=[[ 54.02140906]]\n",
      "Current iteration=270, the loss=[[ 52.63693674]]\n",
      "Current iteration=280, the loss=[[ 50.28852569]]\n",
      "Current iteration=290, the loss=[[ 54.10010935]]\n",
      "Current iteration=300, the loss=[[ 47.44593635]]\n",
      "Current iteration=310, the loss=[[ 46.16550703]]\n",
      "Current iteration=320, the loss=[[ 52.59008971]]\n",
      "Current iteration=330, the loss=[[ 53.61725884]]\n",
      "Current iteration=340, the loss=[[ 53.69376278]]\n",
      "Current iteration=350, the loss=[[ 52.8693746]]\n",
      "Current iteration=360, the loss=[[ 57.15383284]]\n",
      "Current iteration=370, the loss=[[ 50.41824469]]\n",
      "Current iteration=380, the loss=[[ 49.38639328]]\n",
      "Current iteration=390, the loss=[[ 50.12842489]]\n",
      "Current iteration=400, the loss=[[ 54.52280005]]\n",
      "Current iteration=410, the loss=[[ 45.62644529]]\n",
      "Current iteration=420, the loss=[[ 45.2530324]]\n",
      "Current iteration=430, the loss=[[ 58.21331916]]\n",
      "Current iteration=440, the loss=[[ 51.13731105]]\n",
      "Current iteration=450, the loss=[[ 48.86520624]]\n",
      "Current iteration=460, the loss=[[ 49.94964364]]\n",
      "Current iteration=470, the loss=[[ 67.6530765]]\n",
      "Current iteration=480, the loss=[[ 59.19610422]]\n",
      "Current iteration=490, the loss=[[ 51.94975729]]\n",
      "Current iteration=500, the loss=[[ 57.91967023]]\n",
      "Current iteration=510, the loss=[[ 55.01253847]]\n",
      "Current iteration=520, the loss=[[ 53.11137442]]\n",
      "Current iteration=530, the loss=[[ 50.22861192]]\n",
      "Current iteration=540, the loss=[[ 58.03774803]]\n",
      "Current iteration=550, the loss=[[ 46.25429834]]\n",
      "Current iteration=560, the loss=[[ 56.89369923]]\n",
      "Current iteration=570, the loss=[[ 51.23687285]]\n",
      "Current iteration=580, the loss=[[ 58.23395701]]\n",
      "Current iteration=590, the loss=[[ 50.79422323]]\n",
      "Current iteration=600, the loss=[[ 56.2311773]]\n",
      "Current iteration=610, the loss=[[ 49.97049027]]\n",
      "Current iteration=620, the loss=[[ 54.48608694]]\n",
      "Current iteration=630, the loss=[[ 52.04007632]]\n",
      "Current iteration=640, the loss=[[ 42.46060101]]\n",
      "Current iteration=650, the loss=[[ 53.84937848]]\n",
      "Current iteration=660, the loss=[[ 45.9670383]]\n",
      "Current iteration=670, the loss=[[ 54.83021928]]\n",
      "Current iteration=680, the loss=[[ 54.42477166]]\n",
      "Current iteration=690, the loss=[[ 58.01261936]]\n",
      "Current iteration=700, the loss=[[ 58.30775948]]\n",
      "Current iteration=710, the loss=[[ 43.60579388]]\n",
      "Current iteration=720, the loss=[[ 54.77446675]]\n",
      "Current iteration=730, the loss=[[ 54.26041687]]\n",
      "Current iteration=740, the loss=[[ 50.77163524]]\n",
      "Current iteration=750, the loss=[[ 50.56523705]]\n",
      "Current iteration=760, the loss=[[ 52.57697722]]\n",
      "Current iteration=770, the loss=[[ 48.76045991]]\n",
      "Current iteration=780, the loss=[[ 44.96117707]]\n",
      "Current iteration=790, the loss=[[ 58.260242]]\n",
      "Current iteration=800, the loss=[[ 52.72576345]]\n",
      "Current iteration=810, the loss=[[ 59.53520589]]\n",
      "Current iteration=820, the loss=[[ 52.3324724]]\n",
      "Current iteration=830, the loss=[[ 42.21622743]]\n",
      "Current iteration=840, the loss=[[ 47.52691228]]\n",
      "Current iteration=850, the loss=[[ 48.57861061]]\n",
      "Current iteration=860, the loss=[[ 62.10727443]]\n",
      "Current iteration=870, the loss=[[ 49.66550289]]\n",
      "Current iteration=880, the loss=[[ 48.19552904]]\n",
      "Current iteration=890, the loss=[[ 60.44802956]]\n",
      "Current iteration=900, the loss=[[ 52.20486874]]\n",
      "Current iteration=910, the loss=[[ 46.01540061]]\n",
      "Current iteration=920, the loss=[[ 62.42961026]]\n",
      "Current iteration=930, the loss=[[ 51.13568456]]\n",
      "Current iteration=940, the loss=[[ 51.53458325]]\n",
      "Current iteration=950, the loss=[[ 53.20127863]]\n",
      "Current iteration=960, the loss=[[ 48.62370376]]\n",
      "Current iteration=970, the loss=[[ 49.54149761]]\n",
      "Current iteration=980, the loss=[[ 61.04175669]]\n",
      "Current iteration=990, the loss=[[ 43.02702759]]\n",
      "Current iteration=1000, the loss=[[ 49.35648684]]\n",
      "Current iteration=1010, the loss=[[ 40.14185543]]\n",
      "Current iteration=1020, the loss=[[ 51.77907133]]\n",
      "Current iteration=1030, the loss=[[ 47.52246561]]\n",
      "Current iteration=1040, the loss=[[ 49.57252139]]\n",
      "Current iteration=1050, the loss=[[ 58.85743058]]\n",
      "Current iteration=1060, the loss=[[ 47.98137213]]\n",
      "Current iteration=1070, the loss=[[ 50.05326353]]\n",
      "Current iteration=1080, the loss=[[ 53.01128964]]\n",
      "Current iteration=1090, the loss=[[ 40.07878875]]\n",
      "Current iteration=1100, the loss=[[ 50.46826126]]\n",
      "Current iteration=1110, the loss=[[ 41.86093505]]\n",
      "Current iteration=1120, the loss=[[ 51.85816194]]\n",
      "Current iteration=1130, the loss=[[ 50.1262328]]\n",
      "Current iteration=1140, the loss=[[ 53.06997978]]\n",
      "Current iteration=1150, the loss=[[ 45.34503294]]\n",
      "Current iteration=1160, the loss=[[ 48.15907635]]\n",
      "Current iteration=1170, the loss=[[ 45.15788353]]\n",
      "Current iteration=1180, the loss=[[ 57.70125989]]\n",
      "Current iteration=1190, the loss=[[ 59.41416006]]\n",
      "Current iteration=1200, the loss=[[ 45.449617]]\n",
      "Current iteration=1210, the loss=[[ 50.74526004]]\n",
      "Current iteration=1220, the loss=[[ 46.61563586]]\n",
      "Current iteration=1230, the loss=[[ 46.35656864]]\n",
      "Current iteration=1240, the loss=[[ 57.99833619]]\n",
      "Current iteration=1250, the loss=[[ 44.90620855]]\n",
      "Current iteration=1260, the loss=[[ 51.42183835]]\n",
      "Current iteration=1270, the loss=[[ 45.72400325]]\n",
      "Current iteration=1280, the loss=[[ 48.22705353]]\n",
      "Current iteration=1290, the loss=[[ 53.66410067]]\n",
      "Current iteration=1300, the loss=[[ 50.32328859]]\n",
      "Current iteration=1310, the loss=[[ 54.48840373]]\n",
      "Current iteration=1320, the loss=[[ 39.80276254]]\n",
      "Current iteration=1330, the loss=[[ 56.36746986]]\n",
      "Current iteration=1340, the loss=[[ 48.38269766]]\n",
      "Current iteration=1350, the loss=[[ 56.01233192]]\n",
      "Current iteration=1360, the loss=[[ 58.58080302]]\n",
      "Current iteration=1370, the loss=[[ 44.97445696]]\n",
      "Current iteration=1380, the loss=[[ 54.33570252]]\n",
      "Current iteration=1390, the loss=[[ 57.20977142]]\n",
      "Current iteration=1400, the loss=[[ 53.49583015]]\n",
      "Current iteration=1410, the loss=[[ 43.61243505]]\n",
      "Current iteration=1420, the loss=[[ 51.5269543]]\n",
      "Current iteration=1430, the loss=[[ 44.56824598]]\n",
      "Current iteration=1440, the loss=[[ 48.62097837]]\n",
      "Current iteration=1450, the loss=[[ 48.12971927]]\n",
      "Current iteration=1460, the loss=[[ 53.09355996]]\n",
      "Current iteration=1470, the loss=[[ 55.30470401]]\n",
      "Current iteration=1480, the loss=[[ 53.23571163]]\n",
      "Current iteration=1490, the loss=[[ 46.74709587]]\n",
      "Current iteration=1500, the loss=[[ 49.85979596]]\n",
      "Current iteration=1510, the loss=[[ 55.7323956]]\n",
      "Current iteration=1520, the loss=[[ 46.28582764]]\n",
      "Current iteration=1530, the loss=[[ 47.69277281]]\n",
      "Current iteration=1540, the loss=[[ 54.48829063]]\n",
      "Current iteration=1550, the loss=[[ 48.30042647]]\n",
      "Current iteration=1560, the loss=[[ 51.17173919]]\n",
      "Current iteration=1570, the loss=[[ 46.96847061]]\n",
      "Current iteration=1580, the loss=[[ 53.88365241]]\n",
      "Current iteration=1590, the loss=[[ 52.8139083]]\n",
      "Current iteration=1600, the loss=[[ 61.49950322]]\n",
      "Current iteration=1610, the loss=[[ 58.55199704]]\n",
      "Current iteration=1620, the loss=[[ 50.51479184]]\n",
      "Current iteration=1630, the loss=[[ 55.56819862]]\n",
      "Current iteration=1640, the loss=[[ 53.15055091]]\n",
      "Current iteration=1650, the loss=[[ 55.77638984]]\n",
      "Current iteration=1660, the loss=[[ 51.92735377]]\n",
      "Current iteration=1670, the loss=[[ 53.90451676]]\n",
      "Current iteration=1680, the loss=[[ 57.58958574]]\n",
      "Current iteration=1690, the loss=[[ 52.29623023]]\n",
      "Current iteration=1700, the loss=[[ 54.74236545]]\n",
      "Current iteration=1710, the loss=[[ 56.65302299]]\n",
      "Current iteration=1720, the loss=[[ 50.28028724]]\n",
      "Current iteration=1730, the loss=[[ 57.38799242]]\n",
      "Current iteration=1740, the loss=[[ 56.81917915]]\n",
      "Current iteration=1750, the loss=[[ 58.8348748]]\n",
      "Current iteration=1760, the loss=[[ 47.31458436]]\n",
      "Current iteration=1770, the loss=[[ 49.65448788]]\n",
      "Current iteration=1780, the loss=[[ 61.40144954]]\n",
      "Current iteration=1790, the loss=[[ 49.88915152]]\n",
      "Current iteration=1800, the loss=[[ 55.64815498]]\n",
      "Current iteration=1810, the loss=[[ 59.93705269]]\n",
      "Current iteration=1820, the loss=[[ 54.17773494]]\n",
      "Current iteration=1830, the loss=[[ 53.15814774]]\n",
      "Current iteration=1840, the loss=[[ 50.31216553]]\n",
      "Current iteration=1850, the loss=[[ 59.28927489]]\n",
      "Current iteration=1860, the loss=[[ 56.7583392]]\n",
      "Current iteration=1870, the loss=[[ 57.44485673]]\n",
      "Current iteration=1880, the loss=[[ 43.59639679]]\n",
      "Current iteration=1890, the loss=[[ 46.20555747]]\n",
      "Current iteration=1900, the loss=[[ 44.84990627]]\n",
      "Current iteration=1910, the loss=[[ 46.46621143]]\n",
      "Current iteration=1920, the loss=[[ 51.74363596]]\n",
      "Current iteration=1930, the loss=[[ 49.16220416]]\n",
      "Current iteration=1940, the loss=[[ 52.81081783]]\n",
      "Current iteration=1950, the loss=[[ 66.44356675]]\n",
      "Current iteration=1960, the loss=[[ 61.52663694]]\n",
      "Current iteration=1970, the loss=[[ 49.89550411]]\n",
      "Current iteration=1980, the loss=[[ 50.80554442]]\n",
      "Current iteration=1990, the loss=[[ 46.31888263]]\n",
      "Current iteration=2000, the loss=[[ 54.71504859]]\n",
      "Current iteration=2010, the loss=[[ 49.35373116]]\n",
      "Current iteration=2020, the loss=[[ 55.12736856]]\n",
      "Current iteration=2030, the loss=[[ 55.18302063]]\n",
      "Current iteration=2040, the loss=[[ 49.31800626]]\n",
      "Current iteration=2050, the loss=[[ 58.36395383]]\n",
      "Current iteration=2060, the loss=[[ 60.01650845]]\n",
      "Current iteration=2070, the loss=[[ 55.45763392]]\n",
      "Current iteration=2080, the loss=[[ 50.78731576]]\n",
      "Current iteration=2090, the loss=[[ 52.54275369]]\n",
      "Current iteration=2100, the loss=[[ 51.44648211]]\n",
      "Current iteration=2110, the loss=[[ 53.52396085]]\n",
      "Current iteration=2120, the loss=[[ 56.42162303]]\n",
      "Current iteration=2130, the loss=[[ 58.79453162]]\n",
      "Current iteration=2140, the loss=[[ 55.5426552]]\n",
      "Current iteration=2150, the loss=[[ 40.78217755]]\n",
      "Current iteration=2160, the loss=[[ 60.23242963]]\n",
      "Current iteration=2170, the loss=[[ 42.81169542]]\n",
      "Current iteration=2180, the loss=[[ 44.08810141]]\n",
      "Current iteration=2190, the loss=[[ 61.21508009]]\n",
      "Current iteration=2200, the loss=[[ 53.99835258]]\n",
      "Current iteration=2210, the loss=[[ 51.09256156]]\n",
      "Current iteration=2220, the loss=[[ 47.24913956]]\n",
      "Current iteration=2230, the loss=[[ 63.61539318]]\n",
      "Current iteration=2240, the loss=[[ 46.11850159]]\n",
      "Current iteration=2250, the loss=[[ 62.44043355]]\n",
      "Current iteration=2260, the loss=[[ 56.48699696]]\n",
      "Current iteration=2270, the loss=[[ 42.12662096]]\n",
      "Current iteration=2280, the loss=[[ 49.10496046]]\n",
      "Current iteration=2290, the loss=[[ 47.96564585]]\n",
      "Current iteration=2300, the loss=[[ 49.1042003]]\n",
      "Current iteration=2310, the loss=[[ 51.81088274]]\n",
      "Current iteration=2320, the loss=[[ 48.50620526]]\n",
      "Current iteration=2330, the loss=[[ 50.99043384]]\n",
      "Current iteration=2340, the loss=[[ 53.36579838]]\n",
      "Current iteration=2350, the loss=[[ 46.55392619]]\n",
      "Current iteration=2360, the loss=[[ 42.83409335]]\n",
      "Current iteration=2370, the loss=[[ 47.98508514]]\n",
      "Current iteration=2380, the loss=[[ 63.08799354]]\n",
      "Current iteration=2390, the loss=[[ 54.90256565]]\n",
      "Current iteration=2400, the loss=[[ 46.32519229]]\n",
      "Current iteration=2410, the loss=[[ 46.6291666]]\n",
      "Current iteration=2420, the loss=[[ 57.54806997]]\n",
      "Current iteration=2430, the loss=[[ 37.67494746]]\n",
      "Current iteration=2440, the loss=[[ 52.65934919]]\n",
      "Current iteration=2450, the loss=[[ 50.49400966]]\n",
      "Current iteration=2460, the loss=[[ 51.47205657]]\n",
      "Current iteration=2470, the loss=[[ 40.10577664]]\n",
      "Current iteration=2480, the loss=[[ 41.66449287]]\n",
      "Current iteration=2490, the loss=[[ 56.79227679]]\n",
      "Current iteration=2500, the loss=[[ 51.97699837]]\n",
      "Current iteration=2510, the loss=[[ 44.99006797]]\n",
      "Current iteration=2520, the loss=[[ 58.85792318]]\n",
      "Current iteration=2530, the loss=[[ 54.44854066]]\n",
      "Current iteration=2540, the loss=[[ 61.06856033]]\n",
      "Current iteration=2550, the loss=[[ 51.53902927]]\n",
      "Current iteration=2560, the loss=[[ 53.07880613]]\n",
      "Current iteration=2570, the loss=[[ 49.02268665]]\n",
      "Current iteration=2580, the loss=[[ 44.29326322]]\n",
      "Current iteration=2590, the loss=[[ 48.08292889]]\n",
      "Current iteration=2600, the loss=[[ 44.79930835]]\n",
      "Current iteration=2610, the loss=[[ 44.14903064]]\n",
      "Current iteration=2620, the loss=[[ 46.97148404]]\n",
      "Current iteration=2630, the loss=[[ 52.76255111]]\n",
      "Current iteration=2640, the loss=[[ 55.52186419]]\n",
      "Current iteration=2650, the loss=[[ 50.66886986]]\n",
      "Current iteration=2660, the loss=[[ 48.73282252]]\n",
      "Current iteration=2670, the loss=[[ 57.1641865]]\n",
      "Current iteration=2680, the loss=[[ 51.42838529]]\n",
      "Current iteration=2690, the loss=[[ 44.0289829]]\n",
      "Current iteration=2700, the loss=[[ 60.90323521]]\n",
      "Current iteration=2710, the loss=[[ 49.40536625]]\n",
      "Current iteration=2720, the loss=[[ 50.56242908]]\n",
      "Current iteration=2730, the loss=[[ 56.45374879]]\n",
      "Current iteration=2740, the loss=[[ 48.68721104]]\n",
      "Current iteration=2750, the loss=[[ 41.91611342]]\n",
      "Current iteration=2760, the loss=[[ 58.866644]]\n",
      "Current iteration=2770, the loss=[[ 48.24340502]]\n",
      "Current iteration=2780, the loss=[[ 47.62612]]\n",
      "Current iteration=2790, the loss=[[ 57.86864079]]\n",
      "Current iteration=2800, the loss=[[ 43.25842831]]\n",
      "Current iteration=2810, the loss=[[ 47.45450799]]\n",
      "Current iteration=2820, the loss=[[ 52.60700144]]\n",
      "Current iteration=2830, the loss=[[ 49.0206869]]\n",
      "Current iteration=2840, the loss=[[ 50.29471947]]\n",
      "Current iteration=2850, the loss=[[ 44.43440108]]\n",
      "Current iteration=2860, the loss=[[ 51.38777227]]\n",
      "Current iteration=2870, the loss=[[ 45.82216337]]\n",
      "Current iteration=2880, the loss=[[ 48.90894218]]\n",
      "Current iteration=2890, the loss=[[ 46.7002595]]\n",
      "Current iteration=2900, the loss=[[ 56.22982066]]\n",
      "Current iteration=2910, the loss=[[ 51.53530006]]\n",
      "Current iteration=2920, the loss=[[ 60.35609791]]\n",
      "Current iteration=2930, the loss=[[ 48.73849565]]\n",
      "Current iteration=2940, the loss=[[ 53.31621537]]\n",
      "Current iteration=2950, the loss=[[ 47.06683571]]\n",
      "Current iteration=2960, the loss=[[ 58.26277292]]\n",
      "Current iteration=2970, the loss=[[ 47.62432651]]\n",
      "Current iteration=2980, the loss=[[ 66.27843855]]\n",
      "Current iteration=2990, the loss=[[ 52.49770217]]\n",
      "Current iteration=3000, the loss=[[ 51.14513853]]\n",
      "Current iteration=3010, the loss=[[ 44.51929373]]\n",
      "Current iteration=3020, the loss=[[ 41.83690579]]\n",
      "Current iteration=3030, the loss=[[ 51.0221757]]\n",
      "Current iteration=3040, the loss=[[ 46.80775123]]\n",
      "Current iteration=3050, the loss=[[ 61.49008006]]\n",
      "Current iteration=3060, the loss=[[ 55.74114443]]\n",
      "Current iteration=3070, the loss=[[ 46.85250678]]\n",
      "Current iteration=3080, the loss=[[ 56.44194594]]\n",
      "Current iteration=3090, the loss=[[ 52.36655504]]\n",
      "Current iteration=3100, the loss=[[ 52.58135111]]\n",
      "Current iteration=3110, the loss=[[ 54.4506745]]\n",
      "Current iteration=3120, the loss=[[ 52.27853843]]\n",
      "Current iteration=3130, the loss=[[ 51.45767042]]\n",
      "Current iteration=3140, the loss=[[ 55.60054013]]\n",
      "Current iteration=3150, the loss=[[ 56.04601116]]\n",
      "Current iteration=3160, the loss=[[ 45.82653985]]\n",
      "Current iteration=3170, the loss=[[ 54.4902774]]\n",
      "Current iteration=3180, the loss=[[ 45.0590746]]\n",
      "Current iteration=3190, the loss=[[ 55.91022272]]\n",
      "Current iteration=3200, the loss=[[ 51.29733595]]\n",
      "Current iteration=3210, the loss=[[ 47.63662296]]\n",
      "Current iteration=3220, the loss=[[ 53.76807069]]\n",
      "Current iteration=3230, the loss=[[ 43.10891943]]\n",
      "Current iteration=3240, the loss=[[ 56.07725394]]\n",
      "Current iteration=3250, the loss=[[ 44.39124639]]\n",
      "Current iteration=3260, the loss=[[ 50.62340796]]\n",
      "Current iteration=3270, the loss=[[ 54.44724934]]\n",
      "Current iteration=3280, the loss=[[ 53.34718978]]\n",
      "Current iteration=3290, the loss=[[ 54.36484857]]\n",
      "Current iteration=3300, the loss=[[ 46.95667464]]\n",
      "Current iteration=3310, the loss=[[ 43.00120145]]\n",
      "Current iteration=3320, the loss=[[ 49.65580134]]\n",
      "Current iteration=3330, the loss=[[ 44.22929265]]\n",
      "Current iteration=3340, the loss=[[ 57.84009869]]\n",
      "Current iteration=3350, the loss=[[ 49.59449598]]\n",
      "Current iteration=3360, the loss=[[ 60.52467789]]\n",
      "Current iteration=3370, the loss=[[ 50.18007612]]\n",
      "Current iteration=3380, the loss=[[ 45.09279036]]\n",
      "Current iteration=3390, the loss=[[ 49.60762648]]\n",
      "Current iteration=3400, the loss=[[ 62.72754302]]\n",
      "Current iteration=3410, the loss=[[ 54.41143349]]\n",
      "Current iteration=3420, the loss=[[ 52.65631526]]\n",
      "Current iteration=3430, the loss=[[ 47.24254818]]\n",
      "Current iteration=3440, the loss=[[ 60.56478489]]\n",
      "Current iteration=3450, the loss=[[ 48.41886451]]\n",
      "Current iteration=3460, the loss=[[ 51.64832268]]\n",
      "Current iteration=3470, the loss=[[ 54.51946618]]\n",
      "Current iteration=3480, the loss=[[ 47.67757725]]\n",
      "Current iteration=3490, the loss=[[ 51.06524295]]\n",
      "Current iteration=3500, the loss=[[ 57.44134037]]\n",
      "Current iteration=3510, the loss=[[ 48.79342941]]\n",
      "Current iteration=3520, the loss=[[ 47.00351889]]\n",
      "Current iteration=3530, the loss=[[ 47.4456666]]\n",
      "Current iteration=3540, the loss=[[ 44.00176174]]\n",
      "Current iteration=3550, the loss=[[ 52.15294696]]\n",
      "Current iteration=3560, the loss=[[ 52.96844021]]\n",
      "Current iteration=3570, the loss=[[ 65.43062255]]\n",
      "Current iteration=3580, the loss=[[ 50.3582096]]\n",
      "Current iteration=3590, the loss=[[ 46.01931792]]\n",
      "Current iteration=3600, the loss=[[ 49.16974005]]\n",
      "Current iteration=3610, the loss=[[ 45.72033018]]\n",
      "Current iteration=3620, the loss=[[ 53.75775764]]\n",
      "Current iteration=3630, the loss=[[ 48.87186714]]\n",
      "Current iteration=3640, the loss=[[ 55.88930108]]\n",
      "Current iteration=3650, the loss=[[ 41.38207028]]\n",
      "Current iteration=3660, the loss=[[ 42.74798479]]\n",
      "Current iteration=3670, the loss=[[ 49.68019444]]\n",
      "Current iteration=3680, the loss=[[ 44.67774409]]\n",
      "Current iteration=3690, the loss=[[ 56.97517614]]\n",
      "Current iteration=3700, the loss=[[ 42.33405405]]\n",
      "Current iteration=3710, the loss=[[ 60.33483109]]\n",
      "Current iteration=3720, the loss=[[ 50.44688516]]\n",
      "Current iteration=3730, the loss=[[ 51.8157492]]\n",
      "Current iteration=3740, the loss=[[ 46.75644999]]\n",
      "Current iteration=3750, the loss=[[ 47.64392102]]\n",
      "Current iteration=3760, the loss=[[ 54.65010077]]\n",
      "Current iteration=3770, the loss=[[ 51.83087817]]\n",
      "Current iteration=3780, the loss=[[ 48.28502289]]\n",
      "Current iteration=3790, the loss=[[ 57.83639874]]\n",
      "Current iteration=3800, the loss=[[ 55.80778483]]\n",
      "Current iteration=3810, the loss=[[ 51.04130265]]\n",
      "Current iteration=3820, the loss=[[ 43.97619999]]\n",
      "Current iteration=3830, the loss=[[ 56.7480649]]\n",
      "Current iteration=3840, the loss=[[ 49.21282627]]\n",
      "Current iteration=3850, the loss=[[ 49.12368111]]\n",
      "Current iteration=3860, the loss=[[ 54.11870008]]\n",
      "Current iteration=3870, the loss=[[ 45.42904571]]\n",
      "Current iteration=3880, the loss=[[ 48.56353427]]\n",
      "Current iteration=3890, the loss=[[ 46.60432495]]\n",
      "Current iteration=3900, the loss=[[ 50.73361833]]\n",
      "Current iteration=3910, the loss=[[ 40.88127407]]\n",
      "Current iteration=3920, the loss=[[ 55.98299935]]\n",
      "Current iteration=3930, the loss=[[ 52.63549845]]\n",
      "Current iteration=3940, the loss=[[ 49.1330757]]\n",
      "Current iteration=3950, the loss=[[ 52.49513591]]\n",
      "Current iteration=3960, the loss=[[ 52.46511108]]\n",
      "Current iteration=3970, the loss=[[ 54.72201788]]\n",
      "Current iteration=3980, the loss=[[ 45.69232157]]\n",
      "Current iteration=3990, the loss=[[ 45.13171007]]\n",
      "Current iteration=4000, the loss=[[ 62.90405373]]\n",
      "Current iteration=4010, the loss=[[ 46.04735505]]\n",
      "Current iteration=4020, the loss=[[ 42.6503684]]\n",
      "Current iteration=4030, the loss=[[ 50.84735071]]\n",
      "Current iteration=4040, the loss=[[ 47.33730217]]\n",
      "Current iteration=4050, the loss=[[ 50.76164907]]\n",
      "Current iteration=4060, the loss=[[ 46.81801565]]\n",
      "Current iteration=4070, the loss=[[ 41.02453708]]\n",
      "Current iteration=4080, the loss=[[ 55.63057078]]\n",
      "Current iteration=4090, the loss=[[ 48.56327438]]\n",
      "Current iteration=4100, the loss=[[ 51.53090141]]\n",
      "Current iteration=4110, the loss=[[ 56.0223848]]\n",
      "Current iteration=4120, the loss=[[ 53.62202199]]\n",
      "Current iteration=4130, the loss=[[ 46.21232921]]\n",
      "Current iteration=4140, the loss=[[ 49.67299826]]\n",
      "Current iteration=4150, the loss=[[ 57.76372633]]\n",
      "Current iteration=4160, the loss=[[ 51.15385925]]\n",
      "Current iteration=4170, the loss=[[ 49.54480645]]\n",
      "Current iteration=4180, the loss=[[ 48.17799186]]\n",
      "Current iteration=4190, the loss=[[ 63.0937705]]\n",
      "Current iteration=4200, the loss=[[ 52.28565792]]\n",
      "Current iteration=4210, the loss=[[ 55.46125639]]\n",
      "Current iteration=4220, the loss=[[ 51.3864916]]\n",
      "Current iteration=4230, the loss=[[ 48.50301188]]\n",
      "Current iteration=4240, the loss=[[ 54.23802902]]\n",
      "Current iteration=4250, the loss=[[ 49.50115546]]\n",
      "Current iteration=4260, the loss=[[ 50.49262309]]\n",
      "Current iteration=4270, the loss=[[ 44.43013508]]\n",
      "Current iteration=4280, the loss=[[ 39.69345987]]\n",
      "Current iteration=4290, the loss=[[ 48.24816567]]\n",
      "Current iteration=4300, the loss=[[ 55.36237334]]\n",
      "Current iteration=4310, the loss=[[ 55.62716935]]\n",
      "Current iteration=4320, the loss=[[ 62.09612231]]\n",
      "Current iteration=4330, the loss=[[ 58.01011048]]\n",
      "Current iteration=4340, the loss=[[ 48.65849222]]\n",
      "Current iteration=4350, the loss=[[ 51.1535436]]\n",
      "Current iteration=4360, the loss=[[ 55.58225336]]\n",
      "Current iteration=4370, the loss=[[ 48.91955392]]\n",
      "Current iteration=4380, the loss=[[ 47.15575538]]\n",
      "Current iteration=4390, the loss=[[ 43.88141551]]\n",
      "Current iteration=4400, the loss=[[ 50.5608396]]\n",
      "Current iteration=4410, the loss=[[ 43.14448287]]\n",
      "Current iteration=4420, the loss=[[ 39.35839953]]\n",
      "Current iteration=4430, the loss=[[ 55.2274803]]\n",
      "Current iteration=4440, the loss=[[ 43.46728747]]\n",
      "Current iteration=4450, the loss=[[ 43.08896553]]\n",
      "Current iteration=4460, the loss=[[ 46.74239307]]\n",
      "Current iteration=4470, the loss=[[ 54.65254162]]\n",
      "Current iteration=4480, the loss=[[ 46.58196078]]\n",
      "Current iteration=4490, the loss=[[ 62.57968043]]\n",
      "Current iteration=4500, the loss=[[ 49.61948181]]\n",
      "Current iteration=4510, the loss=[[ 43.29248019]]\n",
      "Current iteration=4520, the loss=[[ 57.65759432]]\n",
      "Current iteration=4530, the loss=[[ 49.39437457]]\n",
      "Current iteration=4540, the loss=[[ 64.29288545]]\n",
      "Current iteration=4550, the loss=[[ 53.32904401]]\n",
      "Current iteration=4560, the loss=[[ 49.70127334]]\n",
      "Current iteration=4570, the loss=[[ 53.28736219]]\n",
      "Current iteration=4580, the loss=[[ 43.05731703]]\n",
      "Current iteration=4590, the loss=[[ 47.40207312]]\n",
      "Current iteration=4600, the loss=[[ 52.34414033]]\n",
      "Current iteration=4610, the loss=[[ 51.73942136]]\n",
      "Current iteration=4620, the loss=[[ 51.65130905]]\n",
      "Current iteration=4630, the loss=[[ 49.79745307]]\n",
      "Current iteration=4640, the loss=[[ 62.96956302]]\n",
      "Current iteration=4650, the loss=[[ 45.77819008]]\n",
      "Current iteration=4660, the loss=[[ 46.35877386]]\n",
      "Current iteration=4670, the loss=[[ 41.34154981]]\n",
      "Current iteration=4680, the loss=[[ 46.94420492]]\n",
      "Current iteration=4690, the loss=[[ 53.61457302]]\n",
      "Current iteration=4700, the loss=[[ 55.559327]]\n",
      "Current iteration=4710, the loss=[[ 49.53907741]]\n",
      "Current iteration=4720, the loss=[[ 47.24830386]]\n",
      "Current iteration=4730, the loss=[[ 50.12057076]]\n",
      "Current iteration=4740, the loss=[[ 51.31510723]]\n",
      "Current iteration=4750, the loss=[[ 43.0285716]]\n",
      "Current iteration=4760, the loss=[[ 56.62176628]]\n",
      "Current iteration=4770, the loss=[[ 57.27148517]]\n",
      "Current iteration=4780, the loss=[[ 46.30280837]]\n",
      "Current iteration=4790, the loss=[[ 42.04987033]]\n",
      "Current iteration=4800, the loss=[[ 49.47184635]]\n",
      "Current iteration=4810, the loss=[[ 50.88232687]]\n",
      "Current iteration=4820, the loss=[[ 50.44930296]]\n",
      "Current iteration=4830, the loss=[[ 41.88117239]]\n",
      "Current iteration=4840, the loss=[[ 52.53062373]]\n",
      "Current iteration=4850, the loss=[[ 51.038618]]\n",
      "Current iteration=4860, the loss=[[ 45.81177941]]\n",
      "Current iteration=4870, the loss=[[ 47.58169819]]\n",
      "Current iteration=4880, the loss=[[ 50.48802153]]\n",
      "Current iteration=4890, the loss=[[ 64.47873159]]\n",
      "Current iteration=4900, the loss=[[ 53.57304404]]\n",
      "Current iteration=4910, the loss=[[ 46.00735418]]\n",
      "Current iteration=4920, the loss=[[ 51.75830173]]\n",
      "Current iteration=4930, the loss=[[ 50.86920699]]\n",
      "Current iteration=4940, the loss=[[ 45.70985818]]\n",
      "Current iteration=4950, the loss=[[ 54.5332033]]\n",
      "Current iteration=4960, the loss=[[ 59.16950054]]\n",
      "Current iteration=4970, the loss=[[ 44.80071912]]\n",
      "Current iteration=4980, the loss=[[ 48.64472609]]\n",
      "Current iteration=4990, the loss=[[ 49.31758529]]\n",
      "Weights:\n",
      " [[-0.58736754]\n",
      " [-0.02749412]\n",
      " [-0.7284232 ]\n",
      " [-0.84212051]\n",
      " [ 0.26995709]\n",
      " [-0.0560497 ]\n",
      " [ 0.43468047]\n",
      " [ 0.06539619]\n",
      " [ 0.89478928]\n",
      " [-0.11519225]\n",
      " [ 0.2429707 ]\n",
      " [-0.65376552]\n",
      " [ 0.25806165]\n",
      " [ 0.28440342]\n",
      " [ 0.45356595]\n",
      " [ 0.0111781 ]\n",
      " [ 0.02777539]\n",
      " [ 0.77039743]\n",
      " [ 0.00777291]\n",
      " [ 0.01048842]\n",
      " [ 0.0909573 ]\n",
      " [ 0.04437777]\n",
      " [-0.16676448]\n",
      " [-0.05184549]\n",
      " [-0.27145304]\n",
      " [ 0.03233744]\n",
      " [-0.00203834]\n",
      " [-0.29050203]\n",
      " [ 0.02872626]\n",
      " [ 0.02941405]\n",
      " [ 0.02167009]]\n",
      "\n",
      "\n",
      "Loss:\n",
      " [[ 42.36843176]]\n"
     ]
    }
   ],
   "source": [
    "from implementations import reg_logistic_regression\n",
    "\n",
    "initial_w = np.zeros((processed_tx.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 0.001\n",
    "lambda_ = 0.1\n",
    "\n",
    "w, loss = reg_logistic_regression(y_for_log, processed_tx, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"\\n\")\n",
    "print(\"Loss:\\n\", loss)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
